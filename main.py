import ollama
from database import load_db
import time
import numpy as np

class MCPDispatcher:
    def __init__(self):
     
        self.tools = {
            "rag_search": {
                "keywords": ["v·ªÅ", "l√† g√¨", "t√†i li·ªáu", "th√¥ng tin", "quy ƒë·ªãnh", "m√°y m√≥c", "h∆∞·ªõng d·∫´n"],
                "description": "Truy xu·∫•t ki·∫øn th·ª©c t·ª´ database n·ªôi b·ªô"
            },
            "general_chat": {
                "keywords": ["ch√†o", "hi", "hello", "t·∫°m bi·ªát", "c·∫£m ∆°n", "b·∫°n l√† ai"],
                "description": "T√°n g·∫´u ho·∫∑c ch√†o h·ªèi"
            }
        }

    def fast_route(self, query):
        query_lower = query.lower()
        # ∆Øu ti√™n RAG n·∫øu c√¢u h·ªèi d√†i ho·∫∑c c√≥ t·ª´ kh√≥a chuy√™n m√¥n
        if len(query_lower.split()) > 6:
            return "rag_search"
            
        for tool_name, config in self.tools.items():
            if any(k in query_lower for k in config["keywords"]):
                return tool_name
        
        return "rag_search"

# --- KH·ªûI T·∫†O H·ªÜ TH·ªêNG ---
print("Kh·ªüi t·∫°o h·ªá th·ªëng MCP...")
dispatcher = MCPDispatcher()
try:
    vectorstore = load_db()
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3, "fetch_k": 10})
except:
    retriever = None

def ask_bot():
    while True:
        user_query = input("\nüëâ C√¢u h·ªèi: ").strip()
        if not user_query: continue
        if user_query.lower() in ['exit', 'quit']: break

        t_start = time.perf_counter()

        
        selected_tool = dispatcher.fast_route(user_query)
        
        context_text = ""
        if selected_tool == "rag_search" and retriever:
            print("üîç [Internal Search]...", end="\r")
            docs = retriever.invoke(user_query)
            context_text = "\n\n".join(d.page_content[:150] for d in docs)
            prompt = f"Ng·ªØ c·∫£nh: {context_text}\n\nC√¢u h·ªèi: {user_query}\nTr·∫£ l·ªùi ng·∫Øn g·ªçn:"
        else:
            prompt = user_query

        t_prep = time.perf_counter() - t_start

        print(f"Bot ({selected_tool} | prep: {t_prep:.3f}s): ", end="", flush=True)

        try:
            stream = ollama.chat(
                model="qwen2.5:0.5b",
                messages=[{"role": "user", "content": prompt}],
                stream=True,
                options={"temperature": 0.1}
            )

            for chunk in stream:
                print(chunk['message']['content'], end="", flush=True)
            print()

        except Exception as e:
            print(f"\nL·ªói: {e}")

if __name__ == "__main__":
    ask_bot()