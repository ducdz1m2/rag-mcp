import ollama
from database import load_db
import time
import numpy as np
from functools import lru_cache
import json
import asyncio
from typing import Dict, List, Any, Optional

class MCPDispatcher:
    def __init__(self):
        self.tools = {
            "rag_search": {
                "keywords": ["v·ªÅ", "l√† g√¨", "t√†i li·ªáu", "th√¥ng tin", "quy ƒë·ªãnh", "m√°y m√≥c", "h∆∞·ªõng d·∫´n", "ct", "ƒë·∫°i h·ªçc", "logo", "th∆∞∆°ng hi·ªáu", "e-newsletter", "newsletter", "brand"],
                "description": "Truy xu·∫•t ki·∫øn th·ª©c t·ª´ database n·ªôi b·ªô",
                "handler": self._handle_rag_search
            },
            "sensor_read": {
                "keywords": ["ƒë·ªçc sensor", "ƒë·ªçc d·ªØ li·ªáu", "sensor", "nhi·ªát ƒë·ªô", "ƒë·ªô ·∫©m", "√°nh s√°ng", "ƒë·ªçc", "nhi·ªát", "·∫©m", "s√°ng"],
                "description": "ƒê·ªçc d·ªØ li·ªáu t·ª´ c√°c c·∫£m bi·∫øn",
                "handler": self._handle_sensor_read
            },
            "device_control": {
                "keywords": ["b·∫≠t", "t·∫Øt", "ƒëi·ªÅu khi·ªÉn", "m·ªü", "ƒë√≥ng", "thi·∫øt b·ªã", "qu·∫°t", "ƒë√®n", "relay"],
                "description": "ƒêi·ªÅu khi·ªÉn c√°c thi·∫øt b·ªã",
                "handler": self._handle_device_control
            },
            "general_chat": {
                "keywords": ["ch√†o", "hi", "hello", "t·∫°m bi·ªát", "c·∫£m ∆°n", "b·∫°n l√† ai", "b·∫°n t√™n", "ai"],
                "description": "T√°n g·∫´u ho·∫∑c ch√†o h·ªèi",
                "handler": self._handle_general_chat
            }
        }
        
        # MCP server registry cho m·ªü r·ªông
        self.mcp_servers = {}
        self.register_builtin_servers()

    def register_builtin_servers(self):
        """ƒêƒÉng k√Ω c√°c MCP server builtin"""
        # Placeholder cho c√°c MCP server t∆∞∆°ng lai
        pass
    
    def register_mcp_server(self, name: str, server_config: Dict[str, Any]):
        """ƒêƒÉng k√Ω MCP server m·ªõi"""
        self.mcp_servers[name] = server_config
        print(f"‚úÖ ƒê√£ ƒëƒÉng k√Ω MCP server: {name}")
    
    def route_to_mcp_server(self, tool_name: str, query: str) -> Optional[str]:
        """Route request ƒë·∫øn MCP server t∆∞∆°ng ·ª©ng"""
        if tool_name in self.mcp_servers:
            # G·ªçi MCP server (mock implementation)
            return f"[MCP Response from {tool_name}] ƒê√£ x·ª≠ l√Ω: {query}"
        return None
    
    def _handle_rag_search(self, query: str, retriever) -> str:
        """Handler cho RAG search v·ªõi hybrid search (vector + keyword)"""
        if not retriever:
            return "Xin l·ªói, database t√¨m ki·∫øm ch∆∞a ƒë∆∞·ª£c t·∫£i. Vui l√≤ng ki·ªÉm tra l·∫°i file FAISS index."
        
        try:
            query_lower = query.lower()
            relevant_docs = []
            
            # First try vector search
            docs = retriever.invoke(query)
            
            for d in docs:
                content = d.page_content.strip().lower()
                relevance_score = 0
                
                # Exact phrase matching
                if query_lower in content:
                    relevance_score += 10
                
                # Word matching
                query_words = query_lower.split()
                for word in query_words:
                    if len(word) > 2 and word in content:
                        relevance_score += 2
                
                if relevance_score >= 2:
                    relevant_docs.append((d, relevance_score))
            
            # If no good results, do keyword search across more documents
            if not relevant_docs:
                # Get more documents and filter by keywords
                all_docs = retriever.vectorstore.similarity_search('ƒë·∫°i h·ªçc', k=50)
                
                for d in all_docs:
                    content = d.page_content.strip().lower()
                    relevance_score = 0
                    
                    # More lenient keyword matching
                    query_words = query_lower.split()
                    for word in query_words:
                        if len(word) > 2 and word in content:
                            relevance_score += 1
                    
                    if relevance_score >= 1:
                        relevant_docs.append((d, relevance_score))
            
            # Sort by relevance score
            relevant_docs.sort(key=lambda x: x[1], reverse=True)
            
            if relevant_docs:
                context_parts = []
                for doc, score in relevant_docs[:3]:
                    content = doc.page_content.strip()
                    if len(content) > 50:
                        context_parts.append(content[:500])
                
                context_text = "\n\n".join(context_parts)
                
                return f"D·ª±a tr√™n th√¥ng tin trong database, ƒë√¢y l√† c√¢u tr·∫£ l·ªùi cho c√¢u h·ªèi '{query}':\n\n{context_text}"
            else:
                return f"Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·∫øn '{query}' trong database. Database hi·ªán c√≥ th√¥ng tin v·ªÅ ƒê·∫°i h·ªçc C·∫ßn Th∆°, th∆∞∆°ng hi·ªáu, logo, v√† c√°c t√†i li·ªáu li√™n quan. B·∫°n c√≥ th·ªÉ th·ª≠ h·ªèi v·ªÅ c√°c ch·ªß ƒë·ªÅ n√†y."
                
        except Exception as e:
            return f"Xin l·ªói, c√≥ l·ªói x·∫£y ra khi t√¨m ki·∫øm th√¥ng tin: {str(e)}"
    
    def _handle_general_chat(self, query: str) -> str:
        """Handler cho general chat"""
        query_lower = query.lower()
        
        if any(greeting in query_lower for greeting in ['ch√†o', 'hello', 'hi']):
            return f"Ch√†o b·∫°n! T√¥i l√† AI assistant c·ªßa DTHub. T√¥i c√≥ th·ªÉ gi√∫p b·∫°n t√¨m ki·∫øm th√¥ng tin, ƒë·ªçc sensor data, ho·∫∑c ƒëi·ªÅu khi·ªÉn thi·∫øt b·ªã. B·∫°n c·∫ßn gi√∫p g√¨ kh√¥ng?"
        
        if any(who in query_lower for who in ['b·∫°n l√† ai', 'who are you', 'b·∫°n l√† g√¨']):
            return "T√¥i l√† AI assistant ƒë∆∞·ª£c t√≠ch h·ª£p v√†o h·ªá th·ªëng DTHub. T√¥i c√≥ kh·∫£ nƒÉng x·ª≠ l√Ω c√°c y√™u c·∫ßu v·ªÅ t√¨m ki·∫øm t√†i li·ªáu, ƒë·ªçc d·ªØ li·ªáu c·∫£m bi·∫øn, v√† ƒëi·ªÅu khi·ªÉn thi·∫øt b·ªã IoT th√¥ng qua RAG-MCP system."
        
        if any(thanks in query_lower for thanks in ['c·∫£m ∆°n', 'thank', 'tks']):
            return "R·∫•t vui ƒë∆∞·ª£c gi√∫p ƒë·ª° b·∫°n! N·∫øu c√≥ c√¢u h·ªèi n√†o kh√°c, ƒë·ª´ng ng·∫ßn ng·∫°i h·ªèi nh√©."
        
        if any(bye in query_lower for bye in ['t·∫°m bi·ªát', 'bye', 'goodbye']):
            return "T·∫°m bi·ªát! H·∫πn g·∫∑p l·∫°i b·∫°n s·ªõm."
        
        # Default response
        return f"T√¥i hi·ªÉu b·∫°n n√≥i: '{query}'. T√¥i c√≥ th·ªÉ gi√∫p b·∫°n t√¨m ki·∫øm th√¥ng tin, ƒë·ªçc sensor, ho·∫∑c ƒëi·ªÅu khi·ªÉn thi·∫øt b·ªã. B·∫°n mu·ªën l√†m g√¨ c·ª• th·ªÉ?"
    
    def _handle_sensor_read(self, query: str) -> str:
        """Handler cho sensor reading (placeholder)"""
        # Extract sensor type from query
        query_lower = query.lower()
        
        if 'nhi·ªát ƒë·ªô' in query_lower:
            return f"üå°Ô∏è Nhi·ªát ƒë·ªô hi·ªán t·∫°i: 25.5¬∞C. ƒê·ªçc t·ª´ sensor DHT22 trong ph√≤ng kh√°ch."
        elif 'ƒë·ªô ·∫©m' in query_lower:
            return f"üíß ƒê·ªô ·∫©m hi·ªán t·∫°i: 60%. ƒê·ªçc t·ª´ sensor DHT22 trong ph√≤ng kh√°ch."
        elif '√°nh s√°ng' in query_lower:
            return f"üí° C∆∞·ªùng ƒë·ªô √°nh s√°ng: 800 lux. ƒê·ªçc t·ª´ photoresistor g·∫ßn c·ª≠a s·ªï."
        elif 'soil' in query_lower or 'ƒë·∫•t' in query_lower:
            return f"üå± ƒê·ªô ·∫©m ƒë·∫•t: 45%. ƒê·ªçc t·ª´ soil moisture sensor trong ch·∫≠u c√¢y."
        else:
            # Default sensor reading
            return f"üìä D·ªØ li·ªáu sensors hi·ªán t·∫°i:\n- Nhi·ªát ƒë·ªô: 25.5¬∞C\n- ƒê·ªô ·∫©m: 60%\n- √Ånh s√°ng: 800 lux\n- ƒê·ªô ·∫©m ƒë·∫•t: 45%\n\nTh·ªùi gian ƒë·ªçc: {time.strftime('%H:%M:%S')}"
    
    def _handle_device_control(self, query: str) -> str:
        """Handler cho device control (placeholder)"""
        # Mock response cho device control
        return f"ƒêi·ªÅu khi·ªÉn thi·∫øt b·ªã: {query}. ƒê√£ th·ª±c hi·ªán th√†nh c√¥ng."
    
    def smart_route(self, query: str) -> tuple[str, str]:
        """Smart routing v·ªõi confidence scoring"""
        query_lower = query.lower()
        scores = {}
        
        for tool_name, config in self.tools.items():
            score = 0
            for keyword in config["keywords"]:
                if keyword in query_lower:
                    score += 1
            
            # Bonus cho c√¢u h·ªèi d√†i (RAG)
            if tool_name == "rag_search" and len(query_lower.split()) > 6:
                score += 2
                
            scores[tool_name] = score
        
        # Ch·ªçn tool c√≥ score cao nh·∫•t
        best_tool = max(scores, key=scores.get)
        confidence = scores[best_tool] / max(scores.values()) if max(scores.values()) > 0 else 0
        
        return best_tool, confidence

# --- KH·ªûI T·∫†O H·ªÜ TH·ªêNG ---
print("Kh·ªüi t·∫°o h·ªá th·ªëng MCP...")
dispatcher = MCPDispatcher()

try:
    vectorstore = load_db()
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3, "fetch_k": 8})  # TƒÉng l·∫°i k l√™n 3
except:
    retriever = None

def ask_bot():
    while True:
        user_query = input("\nüëâ C√¢u h·ªèi: ").strip()
        if not user_query: continue
        if user_query.lower() in ['exit', 'quit']: break

        t_start = time.perf_counter()

        
        selected_tool, confidence = dispatcher.smart_route(user_query)
        
        # X·ª≠ l√Ω v·ªõi handler t∆∞∆°ng ·ª©ng
        if selected_tool in dispatcher.tools:
            handler = dispatcher.tools[selected_tool]["handler"]
            
            if selected_tool == "rag_search":
                prompt = handler(user_query, retriever)
                t_search = 0  # S·∫Ω ƒë∆∞·ª£c t√≠nh trong handler
            else:
                prompt = handler(user_query)
                t_search = 0
                
            # Th·ª≠ route ƒë·∫øn MCP server n·∫øu c√≥
            mcp_response = dispatcher.route_to_mcp_server(selected_tool, user_query)
            if mcp_response:
                prompt = f"{prompt}\n\nAdditional MCP Response: {mcp_response}"
        else:
            prompt = user_query
            t_search = 0

        t_prep = time.perf_counter() - t_start

        print(f"Bot ({selected_tool} | conf: {confidence:.2f} | prep: {t_prep:.3f}s): ", end="", flush=True)

        try:
            stream = ollama.chat(
                model="qwen2.5:1.5b",
                messages=[{"role": "user", "content": prompt}],
                stream=True,
                options={
                    "temperature": 0.1,
                    "num_predict": 250  # TƒÉng l√™n 250 ƒë·ªÉ tr·∫£ l·ªùi chi ti·∫øt h∆°n
                }
            )

            for chunk in stream:
                print(chunk['message']['content'], end="", flush=True)
            print()

        except Exception as e:
            print(f"\nL·ªói: {e}")

if __name__ == "__main__":
    ask_bot()