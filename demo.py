import ollama
from database import load_db
import sys
import time

# 1. LOAD DB RA NGO√ÄI V√íNG L·∫∂P: Ch·ªâ load 1 l·∫ßn duy nh·∫•t khi kh·ªüi ƒë·ªông script
print("üöÄ ƒêang kh·ªüi t·∫°o h·ªá th·ªëng v√† load d·ªØ li·ªáu...")
try:
    vectorstore = load_db()
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 3} # Gi·∫£m k xu·ªëng 3 ƒë·ªÉ nhanh h∆°n n·∫øu t√†i li·ªáu ch·∫•t l∆∞·ª£ng
    )
    print("‚úÖ H·ªá th·ªëng s·∫µn s√†ng!")
except Exception as e:
    print(f"‚ùå L·ªói: {e}")
    sys.exit(1)

def ask_bot():
    while True:
        user_query = input("\nüëâ C√¢u h·ªèi: ").strip()
        if user_query.lower() in ['exit', 'quit', '']:
            break

        # B·∫Øt ƒë·∫ßu t√≠nh gi·ªù
        t_start = time.perf_counter()
        
        # Hi·ªÉn th·ªã tr·∫°ng th√°i ngay l·∫≠p t·ª©c
        print("üîç ƒêang t√¨m t√†i li·ªáu...", end="\r", flush=True)
      
        docs = retriever.invoke(user_query)
   
        t_retrieve = time.perf_counter() - t_start

        context_text = "\n\n".join(
            d.page_content[:400].strip()
            for d in docs if d.page_content.strip()
        )

        full_prompt = f"""Tr·∫£ l·ªùi ng·∫Øn g·ªçn d·ª±a tr√™n t√†i li·ªáu:
{context_text}
C√¢u h·ªèi: {user_query}"""

        print(f"ü§ñ Tr·∫£ l·ªùi ({t_retrieve:.2f}s): ", end="", flush=True)

        try:
            stream = ollama.chat(
                model="qwen2.5:0.5b",
                messages=[{"role": "user", "content": full_prompt}],
                stream=True,
                options={
                    "temperature": 0.4, 
                    "keep_alive": "-1",
                }
            )

            for chunk in stream:
                content = chunk['message']['content']
                sys.stdout.write(content)
                sys.stdout.flush()
            print()

        except Exception as e:
            print(f"\nL·ªói: {e}")

if __name__ == "__main__":
    ask_bot()