import os
import fitz  # PyMuPDF (Ph·∫£i c√†i qua pip install pymupdf)
import pytesseract
from PIL import Image
import io
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
# THAY ƒê·ªîI D√íNG N√ÄY:
from langchain_core.documents import Document

# Kh√¥ng c·∫ßn c·∫•u h√¨nh tesseract_cmd tr√™n Fedora v√¨ n√≥ n·∫±m trong /usr/bin/tesseract

DOCS_PATH = "docs/"
VECTOR_DB_PATH = "faiss_index"

def ocr_image_from_page(page):
    ocr_text = ""
    image_list = page.get_images(full=True)
    
    for img_index, img in enumerate(image_list):
        try:
            xref = img[0]
            base_image = page.parent.extract_image(xref)
            image_bytes = base_image["image"]
            
            image = Image.open(io.BytesIO(image_bytes))
            
            # OCR v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u cho ti·∫øng Vi·ªát
            text = pytesseract.image_to_string(image, lang='vie+eng')
            
            if text.strip():
                ocr_text += f"\n[N·ªôi dung t·ª´ h√¨nh ·∫£nh {img_index+1}]:\n{text.strip()}"
        except Exception as e:
            print(f"‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng th·ªÉ OCR h√¨nh ·∫£nh {img_index} tr√™n trang {page.number}: {e}")
            continue
    return ocr_text

def process_pdf_with_ocr(pdf_path):
    doc = fitz.open(pdf_path)
    documents = []
    
    for page_num in range(len(doc)):
        page = doc[page_num]
        
        # 1. L·∫•y text thu·∫ßn
        page_text = page.get_text().strip()
        
        # 2. L·∫•y text t·ª´ ·∫£nh
        image_text = ocr_image_from_page(page)
        
        # N·∫øu l√† trang scan ho√†n to√†n (kh√¥ng c√≥ text thu·∫ßn), ta c√≥ th·ªÉ c√¢n nh·∫Øc 
        # d√πng th√™m t√≠nh nƒÉng convert to√†n b·ªô trang th√†nh ·∫£nh r·ªìi OCR.
        if not page_text and not image_text:
            # Chuy·ªÉn trang th√†nh ·∫£nh (DPI=300 ƒë·ªÉ r√µ n√©t)
            pix = page.get_pixmap(matrix=fitz.Matrix(300/72, 300/72))
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
            page_text = pytesseract.image_to_string(img, lang='vie+eng')

        combined_content = f"{page_text}\n{image_text}".strip()
        
        if combined_content:
            documents.append(Document(
                page_content=combined_content,
                metadata={"source": os.path.basename(pdf_path), "page": page_num + 1}
            ))
            
    doc.close()
    return documents

def process_pdf_simple(pdf_path):
    """Process PDF without OCR - just extract text directly"""
    doc = fitz.open(pdf_path)
    documents = []
    
    for page_num in range(len(doc)):
        page = doc[page_num]
        
        # Only extract text directly from PDF
        page_text = page.get_text().strip()
        
        # Clean up the text but preserve structure
        if page_text:
            # Remove excessive whitespace but keep some line breaks for structure
            lines = page_text.split('\n')
            cleaned_lines = []
            for line in lines:
                line = line.strip()
                if line:  # Skip empty lines
                    cleaned_lines.append(line)
            
            cleaned_text = ' '.join(cleaned_lines)
            
            # Only add if we have meaningful content
            if len(cleaned_text) > 30:  # Lower threshold to catch more content
                documents.append(Document(
                    page_content=cleaned_text,
                    metadata={"source": os.path.basename(pdf_path), "page": page_num + 1}
                ))
            
    doc.close()
    return documents

def build_vector_db():
    print("--- üöÄ PDF Text Extraction with OCR Mode ---")
    
    if not os.path.exists(DOCS_PATH):
        os.makedirs(DOCS_PATH)
        return

    all_docs = []
    pdf_files = [f for f in os.listdir(DOCS_PATH) if f.endswith(".pdf")]
    
    for file in pdf_files:
        print(f"üìÑ ƒêang x·ª≠ l√Ω v·ªõi OCR: {file}...")
        all_docs.extend(process_pdf_with_ocr(os.path.join(DOCS_PATH, file)))

    if not all_docs:
        print("‚ùå Kh√¥ng t√¨m th·∫•y n·ªôi dung n√†o ƒë·ªÉ index!")
        return

    # Increase chunk size for better context
    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)
    texts = splitter.split_documents(all_docs)

    # Use better embedding model for Vietnamese
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    )
    vectorstore = FAISS.from_documents(texts, embeddings)
    vectorstore.save_local(VECTOR_DB_PATH)
    print(f"‚úÖ ƒê√£ l∆∞u th√†nh c√¥ng {len(texts)} chunks!")

if __name__ == "__main__":
    build_vector_db()